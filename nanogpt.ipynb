{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BsnEwEDMYLW"
      },
      "source": [
        "# NanoGPT\n",
        "\n",
        "Authors: Yunhao Cao, Carl(Qianxin Gan), Kevin(Yuxi) Liu\n",
        "\n",
        "In this assignment you will learn to implement a GPT model from scratch. This includes implementing the Transformer architecture (with causal input mask), the GPT model, a byte-level BPE tokenizer, the embedding layer, and the positional encoding layer.\n",
        "We will train a GPT-2 Model that you can play around on your own! Here's the original paper for reference: ([Original Paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf)) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zST1cUo8MYLY"
      },
      "source": [
        "## License Information\n",
        "Copyright 2023 Yunhao Cao, Qianxin Gan, Yuxi Liu\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the â€œSoftwareâ€), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED â€œAS ISâ€, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIehSZqTc_ZC"
      },
      "source": [
        "## Initial Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PK1LtcgedZG5"
      },
      "source": [
        "### Mount Google Drive (skip if not using Colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSC7Lbm1db93",
        "outputId": "d84459a4-95d3-4805-e368-0a6ff18a4a06"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X147PDc6hHNV"
      },
      "source": [
        "### Clone GitHub Repository (skip if not using Colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjSI2zA7fFP-",
        "outputId": "a78c99fa-70c4-466a-8d25-b5dac3e57dfc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "if not os.path.exists(\"CS182_FP_Final\"):\n",
        "  !git clone https://github.com/KevinLiu819/CS182_FP_Final\n",
        "%cd CS182_FP_Final"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqhnNFwKGF94"
      },
      "source": [
        "### Install Python packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OV9FXTy8GH9I",
        "outputId": "d02d88cd-8101-4c30-cfbf-6675909df564"
      },
      "outputs": [],
      "source": [
        "%pip install einops"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqmGQn6AhNWi"
      },
      "source": [
        "### Import Python packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cj-XF_HyfhK-"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "from grader import *\n",
        "import typing\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import einops\n",
        "import math\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Go_t6lTVhX8i"
      },
      "source": [
        "### Set up autograder submitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xaPjUyGlMYLa"
      },
      "outputs": [],
      "source": [
        "# Set up autograder submitter\n",
        "submitter = AutograderSubmitter()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZonZNTV-MYLa"
      },
      "source": [
        "## Part 1: Tokenizer\n",
        "\n",
        "To transform text into a format that can be used by a neural net, we need to first tokenize it (That is, transform the text corpus into indexes of our dictionary). The GPT-2 model uses a byte-level BPE tokenizer. \n",
        "\n",
        "Before we start, please first take a look at [this tutorial video by Huggingface](https://youtu.be/HEikzVL-lZU)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBOe8vF9MYLa"
      },
      "source": [
        "In the tutorial video above, you see that a BPE tokenizer starts with a base dictionary set of characters. Those \"base set of characters\" are usually represented by unicode characters. Unicode characters are the standard way to represent all possible human languages plus our favorite emojis ðŸ˜€ðŸ˜™ in byte streams. There are multiple Unicode standards: UTF-8, UTF-16, UTF-32, etc. However, because unicode characters are not very memory efficient, this basically means that we need to start with a **huge** base dictionary size to begin with our tokenizer. And this is why the authors of GPT-2 chose to instead use a byte-level BPE tokenizer. That is, we split characters into futher smaller fragments (1 byte, or 8-bits) and use those as our base dictionary. This way, we can reduce the base dictionary size from 100,000+ unicode characters to just 256.\n",
        "\n",
        "> As a side note, the UTF-8 standard is not a strict 8-bit-per-character standard and each character in a UTF-8 stream can take more than 8 bits. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7S1LKDRPMYLb"
      },
      "source": [
        "To implement a byte-level BPE tokenizer, we need to first create a base dictionary of 256 characters. This base dictionary will be constructed and passed into our `Dictionary` class via the `__init__` method. Then we need to be able to expand our vocabulary list, one at a time. We will do this by implementing both the `expand_dictionary` method and the `find_combinations_to_expand` method.\n",
        "\n",
        "Basically, think about our current dictionary as all possible alphabets and a space. We would first tokenize our text into a list of indexes in our dictionary. Then we will enumerate through the list of indexes and find the most frequent pair of indexes that appear next to each other. We will then combine those two indexes (of the most frequent pair) into a new index and add it to our dictionary. We will repeat this process until we reach our desired vocabulary size.\n",
        "\n",
        "There are two member attributes of the `Dictionary` class, `dictionary_array` and `combinations_to_index`. The `dictionary_array` attribute simply holds all the vocabularies and `combinations_to_index` is used later in `tokenize()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qm7bCvGXMYLb"
      },
      "outputs": [],
      "source": [
        "class Dictionary:\n",
        "    def __init__(self, base_dictionary : typing.List[bytes] = [i.to_bytes(1,'big') for i in range(256)]) -> None:\n",
        "        \n",
        "        # dictionary holds all volcabulary items and the index of each item in this array will be the input idx to the model\n",
        "        self.dictionary_array : typing.List[bytes] = base_dictionary.copy()\n",
        "\n",
        "        # This is a dictionary that maps a combination of two vocab items to a later vocab item\n",
        "        self.combinations_to_index : typing.Dict[typing.Tuple[int, int], int] = {}\n",
        "    \n",
        "    def __len__(self) -> int:\n",
        "        return len(self.dictionary_array)\n",
        "    \n",
        "    def __getitem__(self, key: int) -> str:\n",
        "        return self.dictionary_array[key]\n",
        "    \n",
        "    def __contains__(self, key: str) -> bool:\n",
        "        return key in self.dictionary_array\n",
        "    \n",
        "    def expand_dictionary(self, combination_vocab : typing.Tuple[int, int]) -> None:\n",
        "        \"\"\"\n",
        "        This function should expand the dictionary with one more vocabulary item, \n",
        "        the item should be the concatenation of the two vocab items in combination_vocab\n",
        "        You need to modify both the dictionary_array and combinations_to_index\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        combination_vocab : typing.Tuple[int, int]\n",
        "            The combination of two vocab items to expand the dictionary with\n",
        "        \"\"\"\n",
        "        # YOUR CODE HERE\n",
        "        # raise NotImplementedError()\n",
        "        self.dictionary_array.append(self.dictionary_array[combination_vocab[0]] + self.dictionary_array[combination_vocab[1]])\n",
        "        self.combinations_to_index[combination_vocab] = len(self.dictionary_array) - 1\n",
        "    \n",
        "\n",
        "    def find_combination_to_expand(self, corpus_of_text: typing.List[int]) -> typing.Tuple[int, int]:\n",
        "        \"\"\"\n",
        "        This function should find the combination of two vocab items that occurs the most in the corpus of text and return it\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        corpus_of_text : typing.List[int]\n",
        "            The corpus of text represented by a list of integers (with each integer representing a vocab in the dictionary) to expand the dictionary with\n",
        "        \"\"\"\n",
        "        # YOUR CODE HERE\n",
        "        # raise NotImplementedError()\n",
        "        count_dict = {}\n",
        "        for i in range(len(corpus_of_text) - 1):\n",
        "            if (corpus_of_text[i], corpus_of_text[i+1]) in count_dict:\n",
        "                count_dict[(corpus_of_text[i], corpus_of_text[i+1])] += 1\n",
        "            else:\n",
        "                count_dict[(corpus_of_text[i], corpus_of_text[i+1])] = 1\n",
        "        return max(count_dict, key=count_dict.get)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3uCv55pMYLc"
      },
      "outputs": [],
      "source": [
        "# Check your implementation\n",
        "grade_dictionary_class_expand_dictionary(Dictionary())\n",
        "grade_dictionary_class_find_combination_to_expand(Dictionary())\n",
        "submitter.submission_data[\"tokenizer_comb\"] = generate_dictionary_class_find_combination_to_expand_dat(\n",
        "    Dictionary()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4q2obZzaMYLc"
      },
      "source": [
        "Awesome! Now we're able to expand our vocabulary list. But how do we tokenize our text into a list of indexes? We will do this by implementing the `tokenize` function. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQ8_Dz7BMYLc"
      },
      "outputs": [],
      "source": [
        "def tokenize(text : str, dictionary : Dictionary) -> typing.List[int]:\n",
        "    \"\"\"\n",
        "    This function should tokenize the text using the dictionary and return the tokenized text as a list of integers\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    text : str\n",
        "        The text to tokenize\n",
        "    \n",
        "    dictionary : Dictionary\n",
        "        The dictionary to use for tokenization\n",
        "    \"\"\"\n",
        "\n",
        "    text_bytestream = bytes(text, \"utf-8\") # convert text to bytestream\n",
        "    tokenized_text : typing.List[int] = [] # initialize tokenized text\n",
        "    for i in range(len(text_bytestream)):\n",
        "        tokenized_text.append(\n",
        "            dictionary.dictionary_array.index(text_bytestream[i:i+1])\n",
        "        )\n",
        "    \n",
        "    num_tokenized_last_pass = len(tokenized_text)\n",
        "    # We will sweep through the tokenized text and replace any combination of two vocab items with the later vocab item\n",
        "    while num_tokenized_last_pass > 0:\n",
        "        # YOUR CODE HERE\n",
        "        # raise NotImplementedError()\n",
        "        num_tokenized_last_pass = 0\n",
        "        new_tokenized_text = []\n",
        "        for i in range(len(tokenized_text) - 1):\n",
        "            if (tokenized_text[i], tokenized_text[i+1]) in dictionary.combinations_to_index:\n",
        "                new_tokenized_text.append(dictionary.combinations_to_index[(tokenized_text[i], tokenized_text[i+1])])\n",
        "                num_tokenized_last_pass += 1\n",
        "            else:\n",
        "                new_tokenized_text.append(tokenized_text[i])\n",
        "                if i == len(tokenized_text) - 2:\n",
        "                    new_tokenized_text.append(tokenized_text[i+1])\n",
        "        \n",
        "        tokenized_text = new_tokenized_text\n",
        "    \n",
        "    return tokenized_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jO-UUQebMYLc"
      },
      "outputs": [],
      "source": [
        "grade_tokenizer(tokenize, Dictionary())\n",
        "submitter.submission_data[\"tokenizer\"] = generate_tokenizer_submission(tokenize, Dictionary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HQYe3MHMYLd"
      },
      "source": [
        "Once you have passed the test, think about why Byte Pair Encoding outperforms many other traditional tokenizers. Write down your answer and include it in your written response. *Hint: think about how BPE handles words that are not in the dictionary (unknown workds) more effectively and how BPE is different in the process of tokenization.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMFemDFIMYLd"
      },
      "source": [
        ">SOLUTION\n",
        "\n",
        "Byte Pair Encoding (BPE) is a subword tokenization algorithm that handles unknown words more effectively compared to other tokenizers. In most traditional tokenizers, if a word is not in the vocabulary, it is treated as an unknown word and assigned a special token, often <UNK>. This results in a loss of information about the unknown word, making it challenging for the model to predict or generate the correct word during the inference process.\n",
        "\n",
        "BPE, on the other hand, alleviates this problem by breaking down words into subword units. It starts by tokenizing the text at the character level and then iteratively merges the most frequent pairs of characters or character sequences to create subwords. This process continues until a predefined vocabulary size is reached or no more merges are possible. The resulting vocabulary consists of a mix of whole words, subword units, and individual characters.\n",
        "\n",
        "When BPE encounters an unknown word, it can break it down into its subword units, which are likely present in the vocabulary. This allows the model to better understand and represent the unknown word, even if it has not seen the exact word during training. By preserving information about the unknown word's structure, BPE enables more accurate predictions and text generation, as the model can leverage the subword information to infer the correct word or generate similar words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AaZEywHMYLd"
      },
      "source": [
        "### Position Embeddings\n",
        "Next we will implement the token embeddings and positional embeddings. The positional embeddings are used to encode the position of each token in the sequence. The positional embeddings are added to the token embeddings before being passed into the Transformer.\n",
        "\n",
        "We will represent our token embeddings as a `torch.nn.Embedding` layer. The positional embeddings will be represented as fixed buffer just like in the original `Attention is all you need` paper.\n",
        "\n",
        "Please notice that formula for position embedding is\n",
        "\n",
        "$$\n",
        "PE_{(pos, 2i)} = \\sin(pos / 10000^{2i / d_{model}})\n",
        "$$\n",
        "\n",
        "$$\n",
        "PE_{(pos, 2i+1)} = \\cos(pos / 10000^{2i / d_{model}})\n",
        "$$\n",
        "\n",
        "where $pos$ is the position and $i$ is the dimension.\n",
        "\n",
        "You just need to implement initilization of the PE matrix here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGiI7v6kMYLd"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model: int, dropout = 0.1, max_len: int = 1024):\n",
        "        super().__init__()\n",
        "        pe = self.calculate_pe(d_model, max_len)\n",
        "        self.register_buffer('pe', pe)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def calculate_pe(self, d_model : int, max_len : int):\n",
        "        \"\"\"\n",
        "        Calculate positional encoding for transformer\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        d_model : int\n",
        "            The dimension of each embedding token\n",
        "        \n",
        "        max_len : int\n",
        "            The maximum length of the sequence\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        pe : Tensor\n",
        "            The positional encoding tensor of shape ``[1, max_len, d_model]``\n",
        "        \"\"\"\n",
        "        # YOUR CODE HERE\n",
        "        # raise NotImplementedError()\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)).unsqueeze(0)\n",
        "        pe = torch.zeros(1, max_len, d_model)\n",
        "        combined_term = position * div_term\n",
        "        pe[0, :, 0::2] = torch.sin(combined_term)\n",
        "        pe[0, :, 1::2] = torch.cos(combined_term)\n",
        "        return pe\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            x: Tensor, shape ``[batch_size, seq_len, embedding_dim]``\n",
        "        \"\"\"\n",
        "        x = x + self.pe[:,:x.shape[1],:]\n",
        "        return self.dropout(x)\n",
        "\n",
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size: int, d_model: int, max_len: int = 1024, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pe = nn.Embedding(max_len, d_model)\n",
        "        # self.pe = PositionalEncoding(d_model, dropout, max_len)\n",
        "\n",
        "    def forward(self, tokens: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            tokens: Tensor, shape ``[batch_size, seq_len]``\n",
        "        \"\"\"\n",
        "        return self.pe(torch.arange(tokens.shape[1], device=tokens.device)) + self.embedding(tokens) # self.pe(self.embedding(tokens) * math.sqrt(self.d_model))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Po0iazw5MYLd"
      },
      "outputs": [],
      "source": [
        "# Check your implementation\n",
        "grade_pe(PositionalEncoding(6,0.0,128))\n",
        "submitter.submission_data[\"PE\"] = generate_pe_sub(PositionalEncoding(12,0.0,256))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGvIJ9gCITxa"
      },
      "source": [
        "## Part 2: Transformer Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_7TtriwMYLd"
      },
      "source": [
        "### Multi-Head Attention\n",
        "\n",
        "Great job on implementing Part 1! In Part 2, we will implement the transformer layer. The first step in building the transformer layer is multi-head attention. Let's understand how attention works. The motivation behind attention is that some parts of our input data are more important than others. We only want to focus on the important parts of the data. The attention mechanism achieves this by using a set of queries, keys, and values generated from the input tokens. A similarity score is produced between a query $q_i$ and a key $k_j$ and higher similiarity scores means token $i$ should attend to token $j$ more. \n",
        "\n",
        "GPT-2 is an auto-regressive model (i.e. it predicts future tokens given previous data). GPT-2 uses masked attention because we want to mask the information about future tokens. If we had information about future tokens, this would be considered cheating because the model would know what to output. GPT-2 uses multi-headed attention, or multiple attention heads. Using multiple heads is useful because it enables the model to attend to parts of the sequence differently. You can think of it as enabling more expressivity in communication amongst tokens.\n",
        "\n",
        "In this section, we implement multi-headed attention head in the following steps:\n",
        "1. Generate queries, keys, and values from the input tokens (computed in the previous part)\n",
        "2. Use the queries, keys, and values to compute masked attention\n",
        "3. Apply a linear projection and dropout to the masked attention\n",
        "\n",
        "Here is how attention is calculated: \n",
        "\n",
        "$$\n",
        "Attention = Softmax(Mask(\\frac{QK^T}{\\sqrt{d_{head}}}))V\n",
        "$$\n",
        "where $d_{head}$ is the embedding dimension of each head, $n_{heads}$ is the number of heads, and $d_{model} = n_{heads} * d_{head}$.\n",
        "\n",
        "**Note:** The input `x` is of size `(batch_size, seq_len, d_model)` and the output is of size `(batch_size, tgt_len, d_model)`. However, the attention function should take in queries of size `(batch_size, n_heads, tgt_len, d_head)` and keys and values of size `(batch_size, n_heads, seq_len, d_head)`. Thus, we need to rearrange the input and attention output somehow to match the different dimensions. Also note that for self-attention, `tgt_len == seq_len` since we are passing the input `x` as the queries, keys, and values.\n",
        "\n",
        "*Hint 1: `einops.rearrange` may be helpful here. You can reference the documentation here: https://einops.rocks/api/rearrange/. We've included the `einops` package for you already.*\n",
        "\n",
        "*Hint 2: To apply the mask before applying softmax, we want to set the weights we want to mask out to $-inf$ so that softmax ignores those weights. We want something like $\\begin{bmatrix} W_{11} & -inf & -inf & ...\\\\ W_{21} & W_{22} & -inf & ...\\\\ W_{31} & W_{32} & W_{33} & ...\\\\ ... & ... & ... & ... \\\\ \\end{bmatrix}$ where $W = \\frac{QK^T}{\\sqrt{d_{head}}}$. `torch.tril` and `torch.Tensor.masked_fill` may be helpful here.*\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4T96qy3-MYLe"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model: int, n_heads: int = 4, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.query_proj = nn.Linear(d_model, d_model)\n",
        "        self.key_proj = nn.Linear(d_model, d_model)\n",
        "        self.value_proj = nn.Linear(d_model, d_model)\n",
        "        self.output_proj = nn.Linear(d_model, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def compute_masked_attention(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Calculate masked attention for transformer\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        Q: Input tensor of queries with size (batch_size, n_heads, tgt_len, d_head)\n",
        "        K: Input tensor of keys with size (batch_size, n_heads, seq_len, d_head)\n",
        "        V: Input tensor of values with size (batch_size, n_heads, seq_len, d_head)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Output attention tensor with size (batch_size, n_heads, tgt_len, d_head)\n",
        "        \"\"\"\n",
        "        # YOUR CODE HERE\n",
        "        # raise NotImplementedError()\n",
        "        _, _, seq_len, d_head = Q.shape\n",
        "        weights = Q @ K.transpose(-2, -1) / (d_head ** 0.5)\n",
        "        mask = torch.tril(torch.ones((seq_len, seq_len), device=Q.device))\n",
        "        masked_weights = weights.masked_fill(mask == 0, float('-inf'))\n",
        "        return nn.functional.softmax(masked_weights, -1) @ V\n",
        "    \n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass for multi-headed attention\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x: Input tensor of tokens with size (batch_size, seq_len, d_model)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Output tensor after forward pass with size (batch_size, tgt_len, d_model)\n",
        "        \"\"\"\n",
        "        # YOUR CODE HERE\n",
        "        # raise NotImplementedError()\n",
        "        Q = self.query_proj(x)\n",
        "        K = self.key_proj(x)\n",
        "        V = self.value_proj(x)\n",
        "        Q = einops.rearrange(Q, 'a b (c d) -> a c b d', c=self.n_heads)\n",
        "        K = einops.rearrange(K, 'a b (c d) -> a c b d', c=self.n_heads)\n",
        "        V = einops.rearrange(V, 'a b (c d) -> a c b d', c=self.n_heads)\n",
        "        attention = self.compute_masked_attention(Q, K, V)\n",
        "        attention = einops.rearrange(attention, 'a c b d -> a b (c d)', c=self.n_heads)\n",
        "        out = self.output_proj(attention)\n",
        "        return self.dropout(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DgeBdPqzMYLe"
      },
      "outputs": [],
      "source": [
        "# Test your implementation\n",
        "grade_attention(MultiHeadAttention)\n",
        "submitter.submission_data[\"Attention\"] = generate_attention_sub(MultiHeadAttention)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPT18aEsMYLe"
      },
      "source": [
        "### Feed-Forward Block\n",
        "\n",
        "Awesome! Now that we have implemented multi-head attention, the next step is to implement the feed-forward block. The feed-forward block is just two linear layers with ReLU in between and dropout at the end. Based on the *Attention is all you Need (Vaswani et al.)* paper, they made the hidden layer dimension of size $4 * d_{model}$ and input and output dimensions of size $d_{model}$. \n",
        "\n",
        "**Note:** GPT-2 uses GeLU activation instead of ReLU activation. GeLU activation is slightly different in that it is smoother than ReLU. You can reference this paper (https://arxiv.org/pdf/1606.08415v3.pdf) to get a better understanding of GeLU, but we have provided the code to calculate GeLU for you already.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kAHIU1hEMYLe"
      },
      "outputs": [],
      "source": [
        "def gelu(x: torch.Tensor) -> torch.Tensor:\n",
        "    return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(d_model, 4 * d_model)\n",
        "        self.gelu = gelu\n",
        "        self.fc2 = nn.Linear(4 * d_model, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass for feed-forward block\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x: Input tensor of tokens with size (batch_size, seq_len, d_model)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Output tensor after forward pass with size (batch_size, seq_len, d_model)\n",
        "        \"\"\"\n",
        "        # YOUR CODE HERE\n",
        "        # raise NotImplementedError()\n",
        "        x = self.fc1(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.dropout(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5kfMrANMYLe"
      },
      "outputs": [],
      "source": [
        "# Test your implementation\n",
        "grade_feed_forward(FeedForward)\n",
        "submitter.submission_data[\"FeedForward\"] = generate_feed_forward_sub(FeedForward)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymnJoAA7MYLf"
      },
      "source": [
        "### Transformer Layer\n",
        "\n",
        "Great! Now we can combine everything into one transformer block. The transformer block involves multi-head attention and the feed-forward step with layer normalizations and residual connections in between. The layer norms and residual connections help prevent vanishing and exploding gradients. We don't use batch norm here because it doesn't make sense to normalize across different examples in a batch for a single feature, especially when the examples can vary in length. \n",
        "\n",
        "Now implement the transformer layer, which is the attention block and feed-forward block with two layer-norms and residual connection in between.\n",
        "\n",
        "**Note:** GPT-2 is a bit different than the *Attention is all you Need (Vaswani et al.)* paper and adds a residual connection after the layer normalization. That is, we compute $residual + LayerNorm(x)$ instead of $LayerNorm(x + residual)$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "faKSmJtMMYLf"
      },
      "outputs": [],
      "source": [
        "class TransformerLayer(nn.Module):\n",
        "    def __init__(self, d_model: int, n_heads: int = 4, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.attention_layer = MultiHeadAttention(d_model, n_heads, dropout)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.feed_forward = FeedForward(d_model, dropout)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "    \n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass for transformer layer\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x: Input tensor of tokens with size (batch_size, seq_len, d_model)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Output tensor after forward pass with size (batch_size, seq_len, d_model)\n",
        "        \"\"\"\n",
        "        # YOUR CODE HERE\n",
        "        # raise NotImplementedError()\n",
        "        attention = self.attention_layer(x)\n",
        "        x = x + self.norm1(attention)\n",
        "        out = self.feed_forward(x)\n",
        "        out = x + self.norm2(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oua4C1WOMYLf"
      },
      "outputs": [],
      "source": [
        "# Test your implementation\n",
        "grade_transformer_layer(TransformerLayer)\n",
        "submitter.submission_data[\"TransformerLayer\"] = generate_transformer_layer_sub(TransformerLayer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEX35sfmMYLf"
      },
      "source": [
        "## Part 3: GPT-2 Model and Training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsCsfUOEMYLf"
      },
      "source": [
        "### GPT-2 Model\n",
        "\n",
        "Awesome work! Now that we have all the essential components, we can proceed to implement the GPT model. The model follows the outlined structure here, but please feel free to tweak with the model afterwards. **Note:** We will be adding softmax later during training, so it is not needed here.\n",
        "\n",
        "1. Token Embedding\n",
        "2. Transformer Layers\n",
        "3. Final Linear Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmLV019mMYLf"
      },
      "outputs": [],
      "source": [
        "class GPT2(nn.Module):\n",
        "    \"\"\"\n",
        "    GPT-2 model implementation.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    vocab_size : int\n",
        "        The size of the vocabulary used in the model.\n",
        "    d_model : int\n",
        "        The dimension of the token embeddings and transformer layers.\n",
        "    n_layers : int\n",
        "        The number of transformer layers in the model.\n",
        "    n_heads : int, optional, default=4\n",
        "        The number of attention heads in the multi-head self-attention mechanism.\n",
        "    dropout : float, optional, default=0.1\n",
        "        The dropout probability for the model.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    token_embedding : TokenEmbedding\n",
        "        The token embedding layer.\n",
        "    transformer_layers : nn.ModuleList\n",
        "        The list of transformer layers.\n",
        "    fc : nn.Linear\n",
        "        The final linear layer that projects the output to the size of the vocabulary.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size: int, d_model: int, n_layers: int, n_heads: int = 4, max_len: int = 1024, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.token_embedding = TokenEmbedding(vocab_size, d_model, max_len, dropout)\n",
        "        self.transformer_layers = nn.ModuleList([TransformerLayer(d_model, n_heads, dropout) for _ in range(n_layers)])\n",
        "        self.fc = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, tokens: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass for the GPT model\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        tokens: torch.Tensor\n",
        "            input tensor of tokens with size (batch_size, seq_len)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        torch.Tensor\n",
        "            Output tensor after forward pass with size (batch_size, seq_len)\n",
        "        \"\"\"\n",
        "        # YOUR CODE HERE\n",
        "        # raise NotImplementedError()\n",
        "        x = self.token_embedding(tokens)\n",
        "        for layer in self.transformer_layers:\n",
        "            x = layer(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test your implementation\n",
        "test_gpt2_model(GPT2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnoL4s46IwIM"
      },
      "source": [
        "### Training\n",
        "\n",
        "In this part, we will implement the training function for our GPT-2 model. We have provided some parts of the code for you, but you will need to implement each training and validation iteration.\n",
        "\n",
        "*Hint 1: Make sure for each training step to run `optimizer.zero_grad()`, `optimizer.step()`, and `train_loss.backward()` to perform backprop. Validation steps do not require this since we don't perform backprop. Training and validation should be very similar.*\n",
        "\n",
        "*Hint 2: You may find `einops.rearrange` to be helpful. It may also be helpful to look into `torch.nn.functional.cross_entropy` documentation (https://pytorch.org/docs/stable/generated/torch.nn.functional.softmax.html).*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ga9wsuB2MYLf"
      },
      "outputs": [],
      "source": [
        "def train_gpt(model, optimizer, train_dataloader, val_dataloader, epochs: int, device='cpu'):\n",
        "    \"\"\"\n",
        "    Train the GPT-2 model.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : GPT2\n",
        "        The GPT-2 model to be trained.\n",
        "    optimizer : torch.optim.Optimizer\n",
        "        The optimizer used for updating the model's weights.\n",
        "    criterion : torch.nn.Module\n",
        "        The loss function used for training.\n",
        "    dataloader : torch.utils.data.DataLoader\n",
        "        The DataLoader used for iterating over the training data.\n",
        "    epochs : int\n",
        "        The number of epochs to train the model.\n",
        "    device : torch.device\n",
        "        The device on which the model will be trained (e.g., 'cpu' or 'cuda'),\n",
        "        defaulted to cpu.\n",
        "    \"\"\"\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        # Training\n",
        "        iters = 0\n",
        "        for batch in train_dataloader:\n",
        "            tokens = batch[\"tokens\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            # START CODE HERE\n",
        "            # raise NotImplementedError()\n",
        "            logits = model(tokens)\n",
        "            logits = einops.rearrange(logits, 'a b c -> a c b')\n",
        "            train_loss = torch.mean(F.cross_entropy(logits, labels)) \n",
        "            optimizer.zero_grad()\n",
        "            train_loss.backward()\n",
        "            optimizer.step()\n",
        "            # END CODE HERE\n",
        "\n",
        "            iters += 1\n",
        "            if iters == 100:\n",
        "                break\n",
        "\n",
        "        # Validation\n",
        "        iters = 0\n",
        "        with torch.no_grad():\n",
        "            for batch in val_dataloader:\n",
        "                tokens = batch[\"tokens\"].to(device)\n",
        "                labels = batch[\"labels\"].to(device)\n",
        "\n",
        "                # START CODE HERE\n",
        "                # raise NotImplementedError()\n",
        "                logits = model(tokens)\n",
        "                logits = einops.rearrange(logits, 'a b c -> a c b')\n",
        "                val_loss = torch.mean(F.cross_entropy(logits, labels))\n",
        "                # END CODE HERE\n",
        "                \n",
        "                iters += 1\n",
        "                if iters == 100:\n",
        "                    break\n",
        "\n",
        "        # Logging\n",
        "        train_losses.append(train_loss.item())\n",
        "        val_losses.append(val_loss.item())\n",
        "        print(f\"Epoch: {epoch + 1}/{epochs}, Training Loss: {train_loss.item()}, Validation Loss: {val_loss.item()}\")\n",
        "    \n",
        "    return train_losses, val_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test your implementation\n",
        "test_train_gpt2(GPT2, train_gpt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsYhTh3cMYLg"
      },
      "source": [
        "## Part 4: Training GPT to Generate Shakespeare\n",
        "\n",
        "Now that we have implemented the entire GPT-2 model and training, we are ready to see its applications! In this part, we will be training our GPT-2 model from scratch to generate \"Shakespeare-like\" text. We will be feeding in a text file of Shakespeare's works. To look into the contents of the text file you can open `data/tiny_shakespeare.txt`. Your task in this part will be to finetune the model hyper-parameters to produce somewhat comprehensible \"Shakespeare-like\" text. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1nzFTB1MYLg"
      },
      "source": [
        "### Load the data and helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PCiCvS_kMYLg"
      },
      "outputs": [],
      "source": [
        "# Load the data\n",
        "with open(\"data/tiny_shakespeare.txt\", \"r\") as f:\n",
        "    data = f.read()\n",
        "n = len(data)\n",
        "vocab = sorted(list(set(data)))\n",
        "vocab_size = len(vocab) # 65\n",
        "dictionary = { ch: i for i, ch in enumerate(vocab) }\n",
        "tokenize_shakespeare = lambda s: [dictionary[c] for c in s]\n",
        "split_train_val = 0.9 # 90% training data, remaining 10% validation data\n",
        "train_data = torch.tensor(tokenize_shakespeare(data[:int(split_train_val * n)]), dtype=torch.long)\n",
        "val_data = torch.tensor(tokenize_shakespeare(data[int(split_train_val * n):]), dtype=torch.long)\n",
        "\n",
        "# Data loader class\n",
        "class ShakespeareDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, batch_size, block_size, use_train = True):\n",
        "        self.batch_size = batch_size\n",
        "        self.block_size = block_size\n",
        "        self.use_train = use_train\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.use_train:\n",
        "            return len(train_data) - self.block_size\n",
        "        else:\n",
        "            return len(val_data) - self.block_size\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        dataset = train_data if self.use_train else val_data\n",
        "        x = dataset[idx:idx + self.block_size]\n",
        "        y = dataset[idx + 1:idx + 1 + self.block_size]\n",
        "        return {\"tokens\": x, \"labels\": y}\n",
        "\n",
        "# Generate function\n",
        "def generate(model, num_tokens, block_size, device):\n",
        "    tokens = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "    with torch.no_grad():\n",
        "        for _ in range(num_tokens):\n",
        "            logits = model(tokens[:, -block_size:])\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = nn.functional.softmax(logits, dim=-1)\n",
        "            token = torch.multinomial(probs, num_samples=1)\n",
        "            tokens = torch.cat((tokens, token), dim=-1)\n",
        "    tokens = torch.squeeze(tokens)\n",
        "\n",
        "    text = \"\"\n",
        "    for token in tokens:\n",
        "        text += vocab[token]\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0F-_FLOMYLg"
      },
      "source": [
        "### Set hyper-parameters for training\n",
        "\n",
        "In this part, you can modify the parameters for finetuning and see how the model performs. We recommend scaling up some of the parameters but not by too much, otherwise the model may take too long to train. For this part, make sure you are also running on GPU, otherwise training will be very slow. \n",
        "\n",
        "Note that we do not use accuracy as a metric here because it would be quite low due to the large number of different possible tokens that can be outputted by the model. We measure by average cross-entropy loss because the loss compares the expected output with the softmax of the logits instead, which is smoother.\n",
        "\n",
        "You will be graded based on the final validation loss. You should achieve a final validation loss â‰¤ 2 in order to receive full credit. A final validation loss of â‰¥ 3 will be consider no credit, and any loss between 2 and 3 will be scaled linearly. \n",
        "\n",
        "$$Score = max(0, min(1, 3 - FinalValLoss))$$\n",
        "\n",
        "For reference, our solution is able to achieve about 1.7 validation loss in about 5 minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C4Rr2QCPMYLg"
      },
      "outputs": [],
      "source": [
        "# MODIFY CODE HERE\n",
        "batch_size = 64 # 32\n",
        "vocab_size = 65 # 65\n",
        "d_model = 384 # 256\n",
        "n_layers = 6 # 1\n",
        "n_heads = 6 # 4\n",
        "dropout = 0.2 # 0.1\n",
        "block_size = 100 # 8\n",
        "epochs = 10 # 10\n",
        "weight_decay = 1e-3 # 1e-4\n",
        "learning_rate = 3e-4 # 1e-4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRUXPBezMYLg"
      },
      "source": [
        "### Initialize, train, and test model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "CxDGHyF3MYLh",
        "outputId": "fcca2083-20c1-4891-b462-7b6d659bc1eb"
      },
      "outputs": [],
      "source": [
        "# Initialize model\n",
        "model = GPT2(vocab_size, d_model, n_layers, n_heads, block_size, dropout)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "train_dataset = ShakespeareDataset(batch_size, block_size, True)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataset = ShakespeareDataset(batch_size, block_size, False)\n",
        "val_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Train and test model\n",
        "train_losses, val_losses = train_gpt(model, optimizer, train_dataloader, val_dataloader, epochs, device)\n",
        "final_loss = val_losses[-1]\n",
        "\n",
        "# Save for autograder\n",
        "submitter.submission_data[\"Shakespeare\"] = final_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0AEMUhQkX0F"
      },
      "source": [
        "### Visualize Training and Validation Losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "id": "pd6iCUDakc2T",
        "outputId": "1c796440-f30a-4006-eaa6-a2aff8c4d94c"
      },
      "outputs": [],
      "source": [
        "# Training Loss Graph\n",
        "plt.plot(np.arange(1, epochs + 1), train_losses)\n",
        "plt.title(\"Training Loss Graph\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Training Loss\")\n",
        "plt.show()\n",
        "\n",
        "# Validation Loss Graph\n",
        "plt.plot(np.arange(1, epochs + 1), train_losses)\n",
        "plt.title(\"Validation Loss Graph\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Validation Loss\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiWpnQLNMYLh"
      },
      "source": [
        "### Generate some outputs\n",
        "\n",
        "Here is the fun part! You can see what your model generates after training. Feel free to change the number of tokens and run the code cell multiple times to see the different outputs that can be produced."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFSLrFwGMYLh",
        "outputId": "60624e83-88b3-4b72-ff47-289e25fb834d"
      },
      "outputs": [],
      "source": [
        "num_tokens = 500 # FEEL FREE TO MODIFY THIS VALUE\n",
        "print(generate(model, num_tokens, block_size, device))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2y-64hyibkgv"
      },
      "source": [
        "### Follow-up Questions\n",
        "\n",
        "1. Please upload the 500-token text that your GPT model generates.\n",
        "2. Comment on what you observe about the resulting text (~100 words)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igWHDYmjMYLh"
      },
      "source": [
        "## Analytical Questions\n",
        "\n",
        "Congratulations on successfully implementing your own GPT-2 model! It is now a good time to take a step back and think about GPT-2 as a whole. Answer the following questions in your written submission:\n",
        "1. How does the model size (number of parameters) affect the quality of generated text and the ability to learn different tasks in a zero-shot setting?\n",
        "2. How does GPT-2 compare with other state-of-the-art models, such as BERT, RoBERTa, or T5, in terms of performance on various NLP benchmarks?\n",
        "3. How can GPT-2 be fine-tuned or adapted for specific tasks or domains, such as medical text generation, code generation, or creative writing?\n",
        "4. Why does gpt-2 architecture needs to have masked attention?\n",
        "5. What are the ethical considerations and potential biases present in GPT-2 due to its training on large-scale web data? How can these biases be mitigated or addressed?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5UnWqGRMYLh"
      },
      "source": [
        ">SOLUTION\n",
        "1. The model size (number of parameters) has a significant impact on the quality of generated text and the ability to learn different tasks in a zero-shot setting. Larger models generally perform better on various NLP tasks, as they have more capacity to learn complex patterns and dependencies in the data. However, they may also require more computational resources and memory during training and inference.\n",
        "2. GPT-2 achieves state-of-the-art performance on several NLP benchmarks, but its performance compared to other models like BERT, RoBERTa, or T5 may vary depending on the task. For instance, BERT and RoBERTa may perform better on tasks that require bidirectional context, while GPT-2 excels in generative tasks.\n",
        "3. GPT-2 can be fine-tuned or adapted for specific tasks or domains by training it on task-specific or domain-specific data. This process involves updating the model's weights using a smaller learning rate and a smaller dataset that is relevant to the target task or domain.\n",
        "4. GPT-2 architecture needs masked attention to enforce its autoregressive nature, ensuring that the model predicts tokens based only on the information available up to the current position in the sequence. This helps the model to generate coherent text and learn context-dependent relationships between tokens.\n",
        "5. Ethical considerations and potential biases in GPT-2 arise from its training on large-scale web data, which may contain biased or offensive content. These biases can be mitigated by using techniques like data filtering, bias-aware training, or fine-tuning on carefully curated datasets that minimize bias."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjwEwKX7MYLh"
      },
      "source": [
        "## Gather your submissions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MsBrRxzMYLh"
      },
      "source": [
        "The following code will generate a `submission.npz` file. Please submit this file to Gradescope."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VqdnXurMYLi"
      },
      "outputs": [],
      "source": [
        "submitter.generate_submission_file(\"submission.npz\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vcvgr22FMYLi",
        "outputId": "e748ff94-198c-4271-8b90-eb20fefd4250"
      },
      "outputs": [],
      "source": [
        "# Internal use\n",
        "from grader_internal import Autograder\n",
        "autograder = Autograder()\n",
        "grade = autograder.grade(submitter.submission_data)\n",
        "grade_2 = autograder.grade(np.load(\"submission.npz\"))\n",
        "assert np.all(grade == grade_2)\n",
        "grade_percent = sum(grade) / 11. * 100\n",
        "print(f\"Grade: {sum(grade)}/11.0\")\n",
        "print(\"Grade: %.2f%%\" % (grade_percent))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8XaGMmZMYLi"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "cs182",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
